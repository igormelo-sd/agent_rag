# embedding.py - Popular Base de Dados RAG (VERS√ÉO CORRIGIDA)
import os
import sys
from pathlib import Path
import logging
from typing import List, Dict, Any
import base64
import io
import hashlib
import json
from dotenv import load_dotenv
import time

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

print("üîß Verificando vari√°veis de ambiente...")
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    print("‚ùå OPENAI_API_KEY n√£o encontrada no .env!")
    print("Por favor, crie um arquivo .env na raiz do projeto com:")
    print("OPENAI_API_KEY=sk-seu-token-aqui")
    sys.exit(1)
else:
    print(f"‚úÖ OPENAI_API_KEY encontrada: {api_key[:10]}...")

# Criar fun√ß√£o de retry simples (sem depend√™ncia externa)
def retry_with_exponential_backoff(max_retries=3, base_delay=1):
    def decorator(func):
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise e
                    delay = base_delay * (2 ** attempt)
                    print(f"Erro na tentativa {attempt + 1}: {e}")
                    print(f"Tentando novamente em {delay}s...")
                    time.sleep(delay)
            return None
        return wrapper
    return decorator

# Tentar importar depend√™ncias.
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    import chromadb
    from chromadb.utils import embedding_functions
    from openai import OpenAI
    import fitz  # PyMuPDF
    from PIL import Image
    
    print("‚úÖ Todas as depend√™ncias importadas com sucesso!")
except ImportError as e:
    print(f"‚ùå Erro ao importar depend√™ncias: {e}")
    print("Por favor, execute:")
    print("pip install langchain-text-splitters chromadb openai PyMuPDF Pillow python-dotenv")
    sys.exit(1)

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configura√ß√£o da API OpenAI
try:
    client_openai = OpenAI(api_key=api_key)
    print("‚úÖ Cliente OpenAI inicializado")
except Exception as e:
    print(f"‚ùå Erro ao inicializar cliente OpenAI: {e}")
    sys.exit(1)

# Inicializa√ß√£o do cache
CACHE_DIR = "cache"
Path(CACHE_DIR).mkdir(exist_ok=True)

def encode_image_to_base64(image: Image.Image) -> str:
    """Converte um objeto de imagem PIL para uma string Base64."""
    buffered = io.BytesIO()
    image.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode('utf-8')

@retry_with_exponential_backoff(max_retries=3, base_delay=2)
def describe_image_with_openai(image_base64: str, page_hash: str) -> str:
    """
    Usa a API da OpenAI (GPT-4o) para descrever o conte√∫do de uma imagem com cache.
    """
    cache_path = Path(CACHE_DIR) / f"{page_hash}.json"
    
    if cache_path.exists():
        logger.info(f"  -> Usando cache para a p√°gina {page_hash[:8]}...")
        try:
            with open(cache_path, "r", encoding="utf-8") as f:
                return json.load(f)["description"]
        except Exception as e:
            logger.warning(f"Erro ao ler cache, gerando nova descri√ß√£o: {e}")

    try:
        logger.info(f"  -> Gerando descri√ß√£o visual com OpenAI...")
        response = client_openai.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": (
                                "Descreva detalhadamente o conte√∫do visual desta imagem, "
                                "incluindo todos os gr√°ficos, tabelas, diagramas e texto presente. "
                                "Traduza o texto para portugu√™s se necess√°rio e resuma as informa√ß√µes-chave. "
                                "A descri√ß√£o deve ser o mais completa e objetiva poss√≠vel para ser usada em um sistema de busca."
                            )
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_base64}"
                            },
                        },
                    ],
                }
            ],
            max_tokens=2048,
        )
        description = response.choices[0].message.content

        # Salva a descri√ß√£o no cache
        try:
            with open(cache_path, "w", encoding="utf-8") as f:
                json.dump({"description": description}, f, ensure_ascii=False)
            logger.info(f"  -> Descri√ß√£o salva no cache")
        except Exception as e:
            logger.warning(f"Erro ao salvar cache: {e}")
            
        return description
    except Exception as e:
        logger.error(f"Erro ao descrever a imagem com OpenAI: {e}")
        return "N√£o foi poss√≠vel gerar uma descri√ß√£o para esta imagem."


def test_pdf_access(pdf_path: Path) -> bool:
    """Testa se o PDF pode ser aberto e processado."""
    try:
        logger.info(f"üß™ Testando acesso ao PDF: {pdf_path}")
        doc = fitz.open(pdf_path)
        page_count = doc.page_count
        logger.info(f"‚úÖ PDF acess√≠vel: {page_count} p√°ginas")
        
        # Testar primeira p√°gina
        if page_count > 0:
            page = doc[0]
            text = page.get_text()
            logger.info(f"‚úÖ Texto da primeira p√°gina extra√≠do: {len(text)} caracteres")
            
            # Testar convers√£o de imagem
            pix = page.get_pixmap(dpi=150)
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            logger.info(f"‚úÖ Imagem extra√≠da: {img.width}x{img.height}")
            
        doc.close()
        return True
    except Exception as e:
        logger.error(f"‚ùå Erro ao testar PDF: {e}")
        return False

def load_and_process_multimodal_documents(data_path: str = "data") -> List[Dict[str, Any]]:
    """
    Carrega PDFs, extrai texto e imagens, e gera descri√ß√µes multimodais.
    """
    logger.info(f"üîç Verificando arquivos PDF em: {os.path.abspath(data_path)}")
    data_dir = Path(data_path)
    
    if not data_dir.exists():
        logger.error(f"‚ùå Diret√≥rio n√£o encontrado: {data_path}")
        logger.info(f"Criando diret√≥rio: {data_path}")
        data_dir.mkdir(parents=True, exist_ok=True)
        return []

    pdf_files = list(data_dir.glob("*.pdf"))
    logger.info(f"üìÅ Arquivos PDF encontrados: {len(pdf_files)}")
    for pdf in pdf_files:
        logger.info(f"  - {pdf.name} ({pdf.stat().st_size / 1024:.1f} KB)")
    
    if not pdf_files:
        logger.error(f"‚ùå Nenhum PDF encontrado em {data_path}")
        return []

    documents = []
    total_pages = 0
    
    for pdf_path in pdf_files:
        logger.info(f"\nüìÑ Processando o arquivo PDF: {pdf_path.name}")
        
        # Testar acesso ao PDF primeiro
        if not test_pdf_access(pdf_path):
            logger.error(f"‚ùå Pulando arquivo {pdf_path.name} devido a erro de acesso")
            continue
            
        try:
            doc = fitz.open(pdf_path)
            logger.info(f"  -> PDF aberto com sucesso: {doc.page_count} p√°ginas")
            
            for i, page in enumerate(doc):
                page_num = i + 1
                logger.info(f"  -> Processando p√°gina {page_num}/{doc.page_count}...")
                
                try:
                    # Extrair texto
                    text_content = page.get_text()
                    logger.info(f"     Texto extra√≠do: {len(text_content)} caracteres")
                    
                    # Gerar hash da p√°gina
                    page_data = f"{pdf_path.name}-page-{page_num}-{text_content[:100]}".encode('utf-8')
                    page_hash = hashlib.sha256(page_data).hexdigest()
                    
                    # Extrair imagem da p√°gina
                    logger.info(f"     Convertendo p√°gina para imagem...")
                    pix = page.get_pixmap(dpi=200) 
                    img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

                    # Redimensionar se necess√°rio
                    max_size = 1024
                    if img.width > max_size or img.height > max_size:
                        logger.info(f"     Redimensionando imagem de {img.width}x{img.height}")
                        img.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)
                    
                    # Converter para base64
                    img_base64 = encode_image_to_base64(img)
                    logger.info(f"     Imagem convertida para base64: {len(img_base64)} caracteres")
                    
                    # Gerar descri√ß√£o da imagem
                    img_description = describe_image_with_openai(img_base64, page_hash)
                    logger.info(f"     Descri√ß√£o gerada: {len(img_description)} caracteres")

                    # Combinar conte√∫dos
                    full_content = (
                        f"--- Conte√∫do da P√°gina {page_num} do arquivo '{pdf_path.name}' ---\n\n"
                        f"TEXTO DA P√ÅGINA:\n{text_content}\n\n"
                        f"DESCRI√á√ÉO VISUAL (Gr√°ficos, Imagens, etc.):\n{img_description}\n"
                    )
                    
                    documents.append({
                        "content": full_content,
                        "metadata": {
                            "source": str(pdf_path),
                            "page": page_num,
                            "file_name": pdf_path.name,
                            "content_length": len(full_content),
                            "text_length": len(text_content),
                            "description_length": len(img_description)
                        }
                    })
                    
                    total_pages += 1
                    logger.info(f"     ‚úÖ P√°gina {page_num} processada com sucesso")
                    
                except Exception as e:
                    logger.error(f"     ‚ùå Erro na p√°gina {page_num}: {e}")
                    continue
                    
            doc.close()
            logger.info(f"‚úÖ Arquivo {pdf_path.name} processado completamente")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar o arquivo {pdf_path.name}: {e}")
            continue
    
    logger.info(f"\n‚úÖ RESUMO: {len(documents)} p√°ginas processadas de {total_pages} p√°ginas totais")
    return documents

def test_chromadb_connection(chroma_path: str, collection_name: str) -> bool:
    """Testa a conex√£o com o ChromaDB."""
    try:
        logger.info(f"üß™ Testando conex√£o ChromaDB: {chroma_path}")
        
        # Verificar se o diret√≥rio existe
        if not Path(chroma_path).exists():
            logger.info(f"Criando diret√≥rio ChromaDB: {chroma_path}")
            Path(chroma_path).mkdir(parents=True, exist_ok=True)
        
        chroma_client = chromadb.PersistentClient(path=chroma_path)
        
        # Testar fun√ß√£o de embedding
        ef = embedding_functions.OpenAIEmbeddingFunction(
            api_key=os.environ.get("OPENAI_API_KEY"),
            model_name="text-embedding-3-small"
        )
        
        # Testar cria√ß√£o de cole√ß√£o
        collection = chroma_client.get_or_create_collection(
            name=f"{collection_name}_test", 
            embedding_function=ef
        )
        
        # Testar adi√ß√£o de documento
        collection.add(
            documents=["Teste de conex√£o"],
            metadatas=[{"test": True}],
            ids=["test-id"]
        )
        
        # Testar consulta
        results = collection.query(query_texts=["teste"], n_results=1)
        
        # Limpar teste
        chroma_client.delete_collection(f"{collection_name}_test")
        
        logger.info("‚úÖ ChromaDB funcionando corretamente")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao testar ChromaDB: {e}")
        return False

def process_documents_to_chromadb(data_path: str = "data", chroma_path: str = "chroma_db", collection_name: str = "seade_gecon"):
    """
    Processa documentos PDF multimodais e adiciona ao ChromaDB.
    """
    print(f"\nüöÄ INICIANDO PROCESSAMENTO DE DOCUMENTOS")
    print(f"üìÅ Diret√≥rio de dados: {os.path.abspath(data_path)}")
    print(f"üóÑÔ∏è ChromaDB: {os.path.abspath(chroma_path)}")
    print(f"üìö Cole√ß√£o: {collection_name}")
    
    # Testar ChromaDB primeiro
    if not test_chromadb_connection(chroma_path, collection_name):
        logger.error("‚ùå Falha na conex√£o com ChromaDB. Abortando.")
        return
    
    # Processar documentos
    documents_raw = load_and_process_multimodal_documents(data_path)
    if not documents_raw:
        logger.warning("‚ùå Nenhum documento processado. Finalizando.")
        return

    logger.info(f"\nüìù Dividindo documentos em chunks...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=4000,
        chunk_overlap=500,
        separators=["\n\n", "\n", " ", ""]
    )
    
    docs_to_embed = []
    chunk_count = 0
    
    for doc_idx, doc in enumerate(documents_raw):
        logger.info(f"  -> Processando documento {doc_idx + 1}/{len(documents_raw)}")
        try:
            chunks = text_splitter.split_text(doc['content'])
            logger.info(f"     Gerados {len(chunks)} chunks")
            
            for chunk_idx, chunk in enumerate(chunks):
                chunk_id = f"{doc['metadata']['file_name']}-page{doc['metadata']['page']}-chunk{chunk_idx}"
                
                docs_to_embed.append({
                    "document": chunk,
                    "metadata": {
                        **doc['metadata'],
                        "chunk_id": chunk_id,
                        "chunk_index": chunk_idx,
                        "total_chunks": len(chunks)
                    },
                    "id": hashlib.sha256(chunk_id.encode()).hexdigest()
                })
                chunk_count += 1
                
        except Exception as e:
            logger.error(f"     ‚ùå Erro ao dividir documento: {e}")

    logger.info(f"‚úÖ Total de {len(docs_to_embed)} chunks prontos para embedding.")

    try:
        logger.info(f"\nüóÑÔ∏è Conectando ao ChromaDB...")
        chroma_client = chromadb.PersistentClient(path=chroma_path)
        
        ef = embedding_functions.OpenAIEmbeddingFunction(
            api_key=os.environ.get("OPENAI_API_KEY"),
            model_name="text-embedding-3-small"
        )
        
        # Remover cole√ß√£o existente se existir
        try:
            chroma_client.delete_collection(collection_name)
            logger.info(f"  -> Cole√ß√£o existente '{collection_name}' removida")
        except:
            pass
        
        collection = chroma_client.create_collection(
            name=collection_name, 
            embedding_function=ef
        )
        logger.info(f"  -> Cole√ß√£o '{collection_name}' criada")
        
        # Adicionar documentos em lotes
        batch_size = 50
        total_batches = (len(docs_to_embed) + batch_size - 1) // batch_size
        
        logger.info(f"üì¶ Adicionando {len(docs_to_embed)} documentos em {total_batches} lotes...")
        
        for i in range(0, len(docs_to_embed), batch_size):
            batch_num = (i // batch_size) + 1
            batch_docs = docs_to_embed[i:i + batch_size]
            
            logger.info(f"  -> Processando lote {batch_num}/{total_batches} ({len(batch_docs)} documentos)...")
            
            try:
                texts = [d['document'] for d in batch_docs]
                metadatas = [d['metadata'] for d in batch_docs]
                ids = [d['id'] for d in batch_docs]
                
                collection.add(
                    documents=texts,
                    metadatas=metadatas,
                    ids=ids
                )
                logger.info(f"     ‚úÖ Lote {batch_num} adicionado com sucesso")
                
            except Exception as e:
                logger.error(f"     ‚ùå Erro no lote {batch_num}: {e}")
                continue
        
        # Verificar resultado final
        final_count = collection.count()
        logger.info(f"\nüéâ PROCESSAMENTO CONCLU√çDO!")
        logger.info(f"   üìä Total de documentos na cole√ß√£o: {final_count}")
        
        # Teste de consulta
        logger.info(f"üß™ Testando consulta...")
        test_results = collection.query(
            query_texts=["economia"], 
            n_results=1
        )
        
        if test_results['documents']:
            logger.info(f"   ‚úÖ Consulta teste funcionou: {len(test_results['documents'][0])} resultado(s)")
        else:
            logger.warning(f"   ‚ö†Ô∏è Consulta teste n√£o retornou resultados")
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao processar ChromaDB: {e}")
        raise

def main():
    """Fun√ß√£o principal para executar o processamento."""
    print("üöÄ SISTEMA DE EMBEDDING MULTIMODAL")
    print("=" * 50)
    
    # Verificar estrutura de diret√≥rios
    data_path = "data"
    chroma_path = "chroma_db"
    
    print(f"üìÅ Verificando estrutura de diret√≥rios...")
    print(f"   Data: {os.path.abspath(data_path)} {'‚úÖ' if Path(data_path).exists() else '‚ùå'}")
    print(f"   ChromaDB: {os.path.abspath(chroma_path)} {'‚úÖ' if Path(chroma_path).exists() else '‚ö†Ô∏è (ser√° criado)'}")
    
    if not Path(data_path).exists():
        logger.error(f"‚ùå Diret√≥rio {data_path} n√£o existe!")
        logger.info("Crie o diret√≥rio e adicione seus arquivos PDF.")
        return
    
    # Executar processamento
    try:
        process_documents_to_chromadb(
            data_path=data_path,
            chroma_path=chroma_path,
            collection_name="seade_gecon"
        )
        print(f"\nüéâ PROCESSAMENTO CONCLU√çDO COM SUCESSO!")
        
    except KeyboardInterrupt:
        print(f"\n‚ö†Ô∏è Processamento interrompido pelo usu√°rio")
    except Exception as e:
        print(f"\n‚ùå ERRO DURANTE O PROCESSAMENTO: {e}")
        logger.exception("Detalhes do erro:")

if __name__ == "__main__":
    main()